{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Hyperparameter Tuning Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>Train RMSE</th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>R¬≤</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'subsa...</td>\n",
       "      <td>0.361312</td>\n",
       "      <td>0.369434</td>\n",
       "      <td>0.563701</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 5, 'subsa...</td>\n",
       "      <td>0.080846</td>\n",
       "      <td>0.126258</td>\n",
       "      <td>0.949040</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 7, 'subsam...</td>\n",
       "      <td>0.021335</td>\n",
       "      <td>0.112267</td>\n",
       "      <td>0.959709</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'learning_rate': 0.2, 'max_depth': 5, 'subsam...</td>\n",
       "      <td>0.051304</td>\n",
       "      <td>0.092952</td>\n",
       "      <td>0.972380</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'learning_rate': 0.3, 'max_depth': 3, 'subsam...</td>\n",
       "      <td>0.062154</td>\n",
       "      <td>0.082635</td>\n",
       "      <td>0.978171</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 10, 'min_sa...</td>\n",
       "      <td>0.088816</td>\n",
       "      <td>0.178725</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>Random Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 20, 'min_sa...</td>\n",
       "      <td>0.074677</td>\n",
       "      <td>0.173034</td>\n",
       "      <td>0.904287</td>\n",
       "      <td>Random Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'n_estimators': 200, 'max_depth': 10, 'min_sa...</td>\n",
       "      <td>0.088145</td>\n",
       "      <td>0.178850</td>\n",
       "      <td>0.897744</td>\n",
       "      <td>Random Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'n_estimators': 200, 'max_depth': 20, 'min_sa...</td>\n",
       "      <td>0.070744</td>\n",
       "      <td>0.172718</td>\n",
       "      <td>0.904636</td>\n",
       "      <td>Random Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'n_estimators': 150, 'max_depth': 15, 'min_sa...</td>\n",
       "      <td>0.065686</td>\n",
       "      <td>0.172339</td>\n",
       "      <td>0.905054</td>\n",
       "      <td>Random Forest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Hyperparameters  Train RMSE  Test RMSE  \\\n",
       "0  {'learning_rate': 0.01, 'max_depth': 3, 'subsa...    0.361312   0.369434   \n",
       "1  {'learning_rate': 0.05, 'max_depth': 5, 'subsa...    0.080846   0.126258   \n",
       "2  {'learning_rate': 0.1, 'max_depth': 7, 'subsam...    0.021335   0.112267   \n",
       "3  {'learning_rate': 0.2, 'max_depth': 5, 'subsam...    0.051304   0.092952   \n",
       "4  {'learning_rate': 0.3, 'max_depth': 3, 'subsam...    0.062154   0.082635   \n",
       "5  {'n_estimators': 100, 'max_depth': 10, 'min_sa...    0.088816   0.178725   \n",
       "6  {'n_estimators': 100, 'max_depth': 20, 'min_sa...    0.074677   0.173034   \n",
       "7  {'n_estimators': 200, 'max_depth': 10, 'min_sa...    0.088145   0.178850   \n",
       "8  {'n_estimators': 200, 'max_depth': 20, 'min_sa...    0.070744   0.172718   \n",
       "9  {'n_estimators': 150, 'max_depth': 15, 'min_sa...    0.065686   0.172339   \n",
       "\n",
       "         R¬≤          Model  \n",
       "0  0.563701        XGBoost  \n",
       "1  0.949040        XGBoost  \n",
       "2  0.959709        XGBoost  \n",
       "3  0.972380        XGBoost  \n",
       "4  0.978171        XGBoost  \n",
       "5  0.897887  Random Forest  \n",
       "6  0.904287  Random Forest  \n",
       "7  0.897744  Random Forest  \n",
       "8  0.904636  Random Forest  \n",
       "9  0.905054  Random Forest  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "drift = pd.read_csv('data/drift_data.csv')\n",
    "\n",
    "# Prepare full dataset\n",
    "X_full = drift.drop(columns=['outcome'])\n",
    "y_full = drift['outcome']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_full_scaled = scaler.fit_transform(X_full)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full_scaled, y_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define hyperparameter grids for XGBoost and Random Forest\n",
    "param_grid_xgb = [\n",
    "    {\"learning_rate\": 0.01, \"max_depth\": 3, \"subsample\": 0.6, \"reg_alpha\": 0.1, \"reg_lambda\": 0.1},\n",
    "    {\"learning_rate\": 0.05, \"max_depth\": 5, \"subsample\": 0.8, \"reg_alpha\": 0.1, \"reg_lambda\": 1},\n",
    "    {\"learning_rate\": 0.1, \"max_depth\": 7, \"subsample\": 1.0, \"reg_alpha\": 0, \"reg_lambda\": 0.1},\n",
    "    {\"learning_rate\": 0.2, \"max_depth\": 5, \"subsample\": 0.8, \"reg_alpha\": 1, \"reg_lambda\": 1},\n",
    "    {\"learning_rate\": 0.3, \"max_depth\": 3, \"subsample\": 0.6, \"reg_alpha\": 0.1, \"reg_lambda\": 0.1},\n",
    "]\n",
    "\n",
    "param_grid_rf = [\n",
    "    {\"n_estimators\": 100, \"max_depth\": 10, \"min_samples_split\": 2, \"min_samples_leaf\": 1},\n",
    "    {\"n_estimators\": 100, \"max_depth\": 20, \"min_samples_split\": 5, \"min_samples_leaf\": 2},\n",
    "    {\"n_estimators\": 200, \"max_depth\": 10, \"min_samples_split\": 2, \"min_samples_leaf\": 1},\n",
    "    {\"n_estimators\": 200, \"max_depth\": 20, \"min_samples_split\": 5, \"min_samples_leaf\": 1},\n",
    "    {\"n_estimators\": 150, \"max_depth\": 15, \"min_samples_split\": 3, \"min_samples_leaf\": 1},\n",
    "]\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"XGBoost\": xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Train models with different hyperparameters and store results\n",
    "results = []\n",
    "\n",
    "for params in param_grid_xgb:\n",
    "    model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    results.append({\"Hyperparameters\": params, \"Train RMSE\": train_rmse, \"Test RMSE\": test_rmse, \"R¬≤\": r2, \"Model\": \"XGBoost\"})\n",
    "\n",
    "for params in param_grid_rf:\n",
    "    model = RandomForestRegressor(random_state=42, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    results.append({\"Hyperparameters\": params, \"Train RMSE\": train_rmse, \"Test RMSE\": test_rmse, \"R¬≤\": r2, \"Model\": \"Random Forest\"})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nüîß Hyperparameter Tuning Results:\")\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Best Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Model</th>\n",
       "      <th>Best Hyperparameters</th>\n",
       "      <th>Train RMSE</th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>R¬≤</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>{'learning_rate': 0.3, 'max_depth': 3, 'subsam...</td>\n",
       "      <td>0.062154</td>\n",
       "      <td>0.082635</td>\n",
       "      <td>0.978171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Best Model                               Best Hyperparameters  Train RMSE  \\\n",
       "0    XGBoost  {'learning_rate': 0.3, 'max_depth': 3, 'subsam...    0.062154   \n",
       "\n",
       "   Test RMSE        R¬≤  \n",
       "0   0.082635  0.978171  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the best model based on the lowest Test RMSE\n",
    "best_model_row = results_df.loc[results_df[\"Test RMSE\"].idxmin()]\n",
    "best_model_params = best_model_row[\"Hyperparameters\"]\n",
    "best_model_name = best_model_row[\"Model\"]\n",
    "\n",
    "# Best model summary\n",
    "best_model_summary = pd.DataFrame({\n",
    "    \"Best Model\": [best_model_name],\n",
    "    \"Best Hyperparameters\": [best_model_params],\n",
    "    \"Train RMSE\": [best_model_row[\"Train RMSE\"]],\n",
    "    \"Test RMSE\": [best_model_row[\"Test RMSE\"]],\n",
    "    \"R¬≤\": [best_model_row[\"R¬≤\"]]\n",
    "})\n",
    "\n",
    "print(\"\\nüèÜ Best Model Summary:\")\n",
    "best_model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Hyperparameter Testing Results:\n",
      "                                     Hyperparameters  Train RMSE  Test RMSE  \\\n",
      "0  {'model': 'Random Forest', 'n_estimators': 100...    0.088816   0.178725   \n",
      "1  {'model': 'Random Forest', 'n_estimators': 200...    0.073816   0.172568   \n",
      "2  {'model': 'Random Forest', 'n_estimators': 150...    0.070478   0.173639   \n",
      "3  {'model': 'Random Forest', 'n_estimators': 250...    0.071083   0.172012   \n",
      "4  {'model': 'Random Forest', 'n_estimators': 300...    0.070473   0.172220   \n",
      "5  {'model': 'XGBoost', 'n_estimators': 100, 'lea...    0.362983   0.372160   \n",
      "6  {'model': 'XGBoost', 'n_estimators': 200, 'lea...    0.041842   0.086032   \n",
      "7  {'model': 'XGBoost', 'n_estimators': 150, 'lea...    0.073831   0.110824   \n",
      "8  {'model': 'XGBoost', 'n_estimators': 250, 'lea...    0.030718   0.097688   \n",
      "9  {'model': 'XGBoost', 'n_estimators': 300, 'lea...    0.029696   0.093286   \n",
      "\n",
      "         R¬≤          Model  \n",
      "0  0.897887  Random Forest  \n",
      "1  0.904801  Random Forest  \n",
      "2  0.903617  Random Forest  \n",
      "3  0.905414  Random Forest  \n",
      "4  0.905185  Random Forest  \n",
      "5  0.557240        XGBoost  \n",
      "6  0.976339        XGBoost  \n",
      "7  0.960738        XGBoost  \n",
      "8  0.969494        XGBoost  \n",
      "9  0.972181        XGBoost  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 104\u001b[0m\n\u001b[0;32m    101\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreg:squarederror\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m    103\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(model, param_grid, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m\"\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 104\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m    106\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m    107\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\hrith\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hrith\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\hrith\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\hrith\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    917\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    918\u001b[0m         clone(base_estimator),\n\u001b[0;32m    919\u001b[0m         X,\n\u001b[0;32m    920\u001b[0m         y,\n\u001b[0;32m    921\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    922\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    923\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    924\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    925\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    927\u001b[0m     )\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    929\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    930\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    931\u001b[0m     )\n\u001b[0;32m    932\u001b[0m )\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\hrith\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\hrith\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\hrith\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hrith\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "drift_data = pd.read_csv('data/drift_data.csv')\n",
    "\n",
    "# Split features & target\n",
    "X = drift_data.drop(columns=['outcome'])\n",
    "y = drift_data['outcome']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define hyperparameters for manual testing\n",
    "manual_params = [\n",
    "    {\"model\": \"Random Forest\", \"n_estimators\": 100, \"max_depth\": 10, \"min_samples_split\": 2, \"min_samples_leaf\": 1},\n",
    "    {\"model\": \"Random Forest\", \"n_estimators\": 200, \"max_depth\": 15, \"min_samples_split\": 5, \"min_samples_leaf\": 2},\n",
    "    {\"model\": \"Random Forest\", \"n_estimators\": 150, \"max_depth\": 12, \"min_samples_split\": 3, \"min_samples_leaf\": 1},\n",
    "    {\"model\": \"Random Forest\", \"n_estimators\": 250, \"max_depth\": 20, \"min_samples_split\": 2, \"min_samples_leaf\": 2},\n",
    "    {\"model\": \"Random Forest\", \"n_estimators\": 300, \"max_depth\": 25, \"min_samples_split\": 5, \"min_samples_leaf\": 1},\n",
    "    \n",
    "    {\"model\": \"XGBoost\", \"n_estimators\": 100, \"learning_rate\": 0.01, \"max_depth\": 3, \"subsample\": 0.8, \"reg_alpha\": 0, \"reg_lambda\": 1},\n",
    "    {\"model\": \"XGBoost\", \"n_estimators\": 200, \"learning_rate\": 0.1, \"max_depth\": 5, \"subsample\": 1.0, \"reg_alpha\": 1, \"reg_lambda\": 1},\n",
    "    {\"model\": \"XGBoost\", \"n_estimators\": 150, \"learning_rate\": 0.05, \"max_depth\": 4, \"subsample\": 0.9, \"reg_alpha\": 0.5, \"reg_lambda\": 1},\n",
    "    {\"model\": \"XGBoost\", \"n_estimators\": 250, \"learning_rate\": 0.2, \"max_depth\": 6, \"subsample\": 0.8, \"reg_alpha\": 1, \"reg_lambda\": 2},\n",
    "    {\"model\": \"XGBoost\", \"n_estimators\": 300, \"learning_rate\": 0.05, \"max_depth\": 7, \"subsample\": 0.7, \"reg_alpha\": 0.5, \"reg_lambda\": 2}\n",
    "]\n",
    "\n",
    "# Store results\n",
    "manual_results = []\n",
    "\n",
    "for params in manual_params:\n",
    "    if params[\"model\"] == \"Random Forest\":\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=params[\"n_estimators\"], \n",
    "            max_depth=params[\"max_depth\"], \n",
    "            min_samples_split=params[\"min_samples_split\"], \n",
    "            min_samples_leaf=params[\"min_samples_leaf\"],\n",
    "            random_state=42\n",
    "        )\n",
    "    else:  # XGBoost\n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=params[\"n_estimators\"], \n",
    "            learning_rate=params[\"learning_rate\"], \n",
    "            max_depth=params[\"max_depth\"], \n",
    "            subsample=params[\"subsample\"], \n",
    "            reg_alpha=params[\"reg_alpha\"], \n",
    "            reg_lambda=params[\"reg_lambda\"],\n",
    "            objective=\"reg:squarederror\", \n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    manual_results.append([\n",
    "        params, train_rmse, test_rmse, r2, params[\"model\"]\n",
    "    ])\n",
    "\n",
    "# Convert to DataFrame\n",
    "manual_results_df = pd.DataFrame(manual_results, columns=[\"Hyperparameters\", \"Train RMSE\", \"Test RMSE\", \"R¬≤\", \"Model\"])\n",
    "print(\"Manual Hyperparameter Testing Results:\")\n",
    "print(manual_results_df)\n",
    "\n",
    "# ---- GridSearchCV ---- #\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [10, 15, 20],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2]\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.05],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],\n",
    "    \"reg_alpha\": [0, 0.5, 1],\n",
    "    \"reg_lambda\": [1, 2]\n",
    "}\n",
    "\n",
    "grid_results = {}\n",
    "\n",
    "for model_name, param_grid in [(\"Random Forest\", param_grid_rf), (\"XGBoost\", param_grid_xgb)]:\n",
    "    model = RandomForestRegressor(random_state=42) if model_name == \"Random Forest\" else xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "    \n",
    "    grid_search = GridSearchCV(model, param_grid, scoring=\"neg_mean_squared_error\", cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    grid_results[model_name] = {\n",
    "        \"Best Hyperparameters\": grid_search.best_params_,\n",
    "        \"Test RMSE\": rmse,\n",
    "        \"R¬≤\": r2\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame\n",
    "grid_results_df = pd.DataFrame(grid_results).T\n",
    "print(\"\\nGridSearch Best Model Results:\")\n",
    "print(grid_results_df)\n",
    "\n",
    "# ---- Comparison of Manual Best Model vs GridSearch Best Model ---- #\n",
    "best_manual_model = manual_results_df.sort_values(by=\"Test RMSE\").iloc[0]\n",
    "best_grid_model = grid_results_df.sort_values(by=\"Test RMSE\").iloc[0]\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Manual Best Model\": [best_manual_model[\"Hyperparameters\"]],\n",
    "    \"GridSearch Best Model\": [best_grid_model[\"Best Hyperparameters\"]],\n",
    "    \"Manual RMSE\": [best_manual_model[\"Test RMSE\"]],\n",
    "    \"GridSearch RMSE\": [best_grid_model[\"Test RMSE\"]],\n",
    "    \"Manual R¬≤\": [best_manual_model[\"R¬≤\"]],\n",
    "    \"GridSearch R¬≤\": [best_grid_model[\"R¬≤\"]]\n",
    "})\n",
    "\n",
    "print(\"\\nComparison Between Manual and GridSearch Best Model:\")\n",
    "print(comparison_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
